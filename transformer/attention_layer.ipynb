{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8472d6",
   "metadata": {},
   "source": [
    "## Transformers from scratch mini-project\n",
    "I was curious how transformers worked internally so I wrote one in tensorflow based on [this explanation](https://peterbloem.nl/blog/transformers). In order to check correctness of my implementation I ran this NLP pipeline from [Tensorflow](https://github.com/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb) on my local GPU, and substituted my MultiHeadAttention layer for the one in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce886da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)  # sets seeds for base-python, numpy and tf\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a6a1aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9, 4), dtype=float32, numpy=\n",
       "array([[[0.803156  , 0.49777734, 0.37054038, 0.9118674 ],\n",
       "        [0.637642  , 0.18209696, 0.63791955, 0.27701473],\n",
       "        [0.04227114, 0.84219384, 0.90637195, 0.222556  ],\n",
       "        [0.9198462 , 0.68789077, 0.42705178, 0.878158  ],\n",
       "        [0.6943959 , 0.46567595, 0.52925766, 0.33019018],\n",
       "        [0.12754858, 0.16153514, 0.5085137 , 0.44301772],\n",
       "        [0.35205877, 0.8969147 , 0.24940813, 0.76328313],\n",
       "        [0.85935795, 0.08480155, 0.20418596, 0.28848922],\n",
       "        [0.65142167, 0.7106751 , 0.8695041 , 0.23745108]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1,9,4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e025c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.heads = num_heads\n",
    "        self.dim = d_model\n",
    "        assert self.dim % self.heads == 0\n",
    "        self.depth = self.dim // self.heads\n",
    "      \n",
    "    def build(self, batch_input_shape):\n",
    "        dim = batch_input_shape[-1]\n",
    "        self.Wq = self.add_weight(name=\"Wq\", shape=[dim, dim], initializer=\"glorot_normal\")\n",
    "        self.Wk = self.add_weight(name=\"Wk\", shape=[dim, dim], initializer=\"glorot_normal\")\n",
    "        self.Wv = self.add_weight(name=\"Wv\", shape=[dim, dim], initializer=\"glorot_normal\")\n",
    "        self.Wo = self.add_weight(name=\"Wo\", shape=[dim, dim], initializer=\"glorot_normal\")\n",
    "\n",
    "    def split_into_heads(self, M, batch_size): \n",
    "        M = tf.reshape(M, (batch_size, -1, self.heads, self.depth))\n",
    "        return tf.transpose(M, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask=None):\n",
    "        batch = tf.shape(q)[0]\n",
    "        seq_len = tf.shape(q)[-1]\n",
    "\n",
    "        q = tf.transpose(q, perm=[0, 2, 1])\n",
    "        k = tf.transpose(k, perm=[0, 2, 1]) \n",
    "        v = tf.transpose(v, perm=[0, 2, 1]) \n",
    "\n",
    "        Q = self.Wq @ q # (dim, dim) * (batch, dim, seq_len)\n",
    "        Q = tf.transpose(Q, perm=[0, 2, 1]) #transpose to (batch, seq_len, dim)\n",
    "        Q = self.split_into_heads(Q, batch) #split then transpose -> (batch, heads, seq_len, dim // heads)\n",
    "        \n",
    "        K = self.Wk @ k\n",
    "        K = tf.transpose(K, perm=[0, 2, 1])\n",
    "        K = self.split_into_heads(K, batch)\n",
    "        \n",
    "        V = self.Wv @ v\n",
    "        V = tf.transpose(V, perm=[0, 2, 1])\n",
    "        V = self.split_into_heads(V, batch)\n",
    "        \n",
    "        d = tf.cast(q.shape[1], tf.float32)\n",
    "\n",
    "        scaled_attention_logits = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(d)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        unrolled = attention_weights @ V\n",
    "\n",
    "        unrolled = tf.transpose(unrolled, [0, 2, 1, 3])\n",
    "        concatenated = tf.reshape(unrolled, [batch, -1, self.dim])\n",
    "        return concatenated @ self.Wo, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2a0eac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9, 4), dtype=float32, numpy=\n",
       "array([[[0.35597828, 0.0591852 , 0.06663257, 0.3346224 ],\n",
       "        [0.36085227, 0.03652559, 0.07470929, 0.34728718],\n",
       "        [0.36287993, 0.03878205, 0.07710293, 0.34061083],\n",
       "        [0.35561445, 0.06234346, 0.06531179, 0.33334184],\n",
       "        [0.35947052, 0.04296065, 0.07254462, 0.34330994],\n",
       "        [0.36129224, 0.03525817, 0.07694381, 0.34424722],\n",
       "        [0.35690615, 0.0572939 , 0.06944344, 0.3314358 ],\n",
       "        [0.35885248, 0.03900723, 0.07306662, 0.34727803],\n",
       "        [0.36076185, 0.04316544, 0.0729384 , 0.34268153]]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = MultiHeadAttention(4, 2)\n",
    "m1(x,x,x)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
